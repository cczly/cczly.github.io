---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

I work at Suzhou Institute for advanced research, USTC as a Postdoctoral now in Suzhou.

I am now working on audio-driven talking face generation, three-dimensional reconstruction and multi-modal large model. If you are seeking any form of academic cooperation, please feel free to email me at cczly@ustc.edu.cn. We are hiring interns!

My research interest includes Image/video generation, three-dimensional reconstruction and multi-modal large model. I have published in several journals, such as IEEE Transactions on Image Processing, IEEE Transactions on Multimedia, IEEE Transactions on Circuits and Systems for Video Technology, Pattern Recognition, and IEEE Transactions on Instrumentation and Measurement, and top conferences, such as CVPR, ICCV, AAAI, and ACM MM. I am nominated for the Shanghai Computer Society's Outstanding Doctoral Dissertation Award, received the honor of Outstanding Graduate Student of Shanghai, and was titled as an Outstanding Postdoctoral Fellow in Jiangsu Province, China.

# üî• News
- *2024.06*: &nbsp;üéâ Received the Youth Fund from the Jiangsu Province Basic Research Program.
- *2024.06*: &nbsp;üéâ A journal article is accepted by IEEE TCSVT.
- *2024.01*: &nbsp;üéâ A journal article is accepted by IEEE TIP.
- *2023.12*: &nbsp;üéâ A conference paper is accepted by IEEE ICASSP2024.
- *2023.07*: &nbsp;üéâ Received the Special Fund of The China Postdoctoral Science Foundation. 

# üéñ Honors and Awards
- *2023*:&nbsp;üéâüéâ Outstanding Postdoctoral Fellow in Jiangsu Province.
- *2022*:&nbsp;üéâüéâ Outstanding Graduate Student of Shanghai. 
- *2022*:&nbsp;üéâüéâ Shanghai Computer Society's Outstanding Doctoral Dissertation Award (Nominated).

# $$ Fundings
- *2024.07*: &nbsp;üéâüéâ Youth Fund of Jiangsu Provincial Basic Research Program (Natural Science Foundation).
- *2023.07*: &nbsp;üéâüéâ Special Fund of The China Postdoctoral Science Foundation. 
- *2023.06*: &nbsp;üéâüéâ General Fund of The China Postdoctoral Science Foundation. 
- *2023.06*: &nbsp;üéâüéâ The Jiangsu Funding Program for Excellent Postdoctoral Talent.

# üìù Publications 
**Face Analysis and Synthesis**

<ul>
  <li><code class="language-plaintext highlighter-rouge">AAAI 2024</code> <a href="https://arxiv.org/abs/2312.11947">Emotion Rendering for Conversational Speech Synthesis with Heterogeneous Graph-Based Context Modeling</a>, Rui Liu, Yifan Hu, <strong>Yi Ren</strong>, et al. <a href="https://github.com/walker-hyf/ECSS"><img src="https://img.shields.io/github/stars/walker-hyf/ECSS?style=social&amp;label=Code+Stars" alt="" /></a></li>
  <li><code class="language-plaintext highlighter-rouge">ICML 2023</code> <a href="https://text-to-audio.github.io/paper.pdf">Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models</a>, Rongjie Huang, Jiawei Huang, Dongchao Yang, <strong>Yi Ren</strong>, et al.</li>
  <li><code class="language-plaintext highlighter-rouge">ACL 2023</code> <a href="">CLAPSpeech: Learning Prosody from Text Context with Contrastive Language-Audio Pre-Training</a>, Zhenhui Ye, Rongjie Huang, <strong>Yi Ren</strong>, et al.</li>
  <li><code class="language-plaintext highlighter-rouge">ACL 2023</code> <a href="">FluentSpeech: Stutter-Oriented Automatic Speech Editing with Context-Aware Diffusion Models</a>, Ziyue Jiang, Qian Yang, Jialong Zuo, Zhenhui Ye, Rongjie Huang, <strong>Yi Ren</strong> and Zhou Zhao</li>
  <li><code class="language-plaintext highlighter-rouge">ACL 2023</code> <a href="">Revisiting and Incorporating GAN and Diffusion Models in High-Fidelity Speech Synthesis</a>, Rongjie Huang, <strong>Yi Ren</strong>, Ziyue Jiang, et al.</li>
  <li><code class="language-plaintext highlighter-rouge">ACL 2023</code> <a href="">Improving Prosody with Masked Autoencoder and Conditional Diffusion Model For Expressive Text-to-Speech</a>, Rongjie Huang, Chunlei Zhang, <strong>Yi Ren</strong>, et al.</li>
  <li><code class="language-plaintext highlighter-rouge">ICLR 2023</code> <a href="https://openreview.net/forum?id=SbR9mpTuBn">Bag of Tricks for Unsupervised Text-to-Speech</a>, <strong>Yi Ren</strong>, Chen Zhang, Shuicheng Yan</li>
  <li><code class="language-plaintext highlighter-rouge">INTERSPEECH 2023</code> <a href="https://arxiv.org/abs/2305.17732">StyleS2ST: zero-shot style transfer for direct speech-to-speech translation</a>, Kun Song, <strong>Yi Ren</strong>, Yi Lei, et al.</li>
  <li><code class="language-plaintext highlighter-rouge">INTERSPEECH 2023</code> <a href="https://arxiv.org/abs/2306.15304">GenerTTS: Pronunciation Disentanglement for Timbre and Style Generalization in Cross-Lingual Text-to-Speech</a>, Yahuan Cong, Haoyu Zhang, Haopeng Lin, Shichao Liu, Chunfeng Wang, <strong>Yi Ren</strong>, et al.</li>
  <li><code class="language-plaintext highlighter-rouge">NeurIPS 2022</code> <a href="">Dict-TTS: Learning to Pronounce with Prior Dictionary Knowledge for Text-to-Speech</a>, Ziyue Jiang, Zhe Su, Zhou Zhao, Qian Yang, <strong>Yi Ren</strong>, et al. <a href="https://github.com/Zain-Jiang/Dict-TTS"><img src="https://img.shields.io/github/stars/Zain-Jiang/Dict-TTS?style=social&amp;label=Code+Stars" alt="" /></a></li>
  <li><code class="language-plaintext highlighter-rouge">NeurIPS 2022</code> <a href="">GenerSpeech: Towards Style Transfer for Generalizable Out-Of-Domain Text-to-Speech</a>, Rongjie Huang, <strong>Yi Ren</strong>, et al.</li>
  <li><code class="language-plaintext highlighter-rouge">NeurIPS 2022</code> <a href="">M4Singer: a Multi-Style, Multi-Singer and Musical Score Provided Mandarin Singing Corpus</a>, Lichao Zhang, Ruiqi Li, Shoutong Wang, Liqun Deng, Jinglin Liu, <strong>Yi Ren</strong>, et al. <em>(Datasets and Benchmarks Track)</em> <a href="https://github.com/M4Singer/M4Singer"><img src="https://img.shields.io/github/stars/M4Singer/M4Singer?style=social&amp;label=Dataset+Stars" alt="" /></a></li>
  <li><code class="language-plaintext highlighter-rouge">ACM-MM 2022</code> <a href="">ProDiff: Progressive Fast Diffusion Model for High-Quality Text-to-Speech</a>, Rongjie Huang, Zhou Zhao, Huadai Liu, Jinglin Liu, Chenye Cui, <strong>Yi Ren</strong>, <a href="https://github.com/Rongjiehuang/ProDiff"><img src="https://img.shields.io/github/stars/Rongjiehuang/ProDiff?style=social&amp;label=Code+Stars" alt="" /></a></li>
  <li><code class="language-plaintext highlighter-rouge">ACM-MM 2022</code> <a href="https://arxiv.org/abs/2110.07468">SingGAN: Generative Adversarial Network For High-Fidelity Singing Voice Generation</a>, Rongjie Huang, Chenye Cui, Chen Feiayng, <strong>Yi Ren</strong>, et al.</li>
  <li><code class="language-plaintext highlighter-rouge">IJCAI 2022</code> <a href="">SyntaSpeech: Syntax-Aware Generative Adversarial Text-to-Speech</a>, Zhenhui Ye, Zhou Zhao, <strong>Yi Ren</strong>, et al. <a href="https://github.com/yerfor/SyntaSpeech"><img src="https://img.shields.io/github/stars/yerfor/SyntaSpeech?style=social&amp;label=Code+Stars" alt="" /></a></li>
  <li><code class="language-plaintext highlighter-rouge">IJCAI 2022</code> <span style="color:red">(Oral)</span> <a href="">EditSinger: Zero-Shot Text-Based Singing Voice Editing System with Diverse Prosody Modeling</a>, Lichao Zhang, Zhou Zhao, <strong>Yi Ren</strong>, et al.</li>
  <li><code class="language-plaintext highlighter-rouge">IJCAI 2022</code> <a href="">FastDiff: A Fast Conditional Diffusion Model for High-Quality Speech Synthesis</a>, Rongjie Huang, Max W. Y. Lam, Jun Wang, Dan Su, Dong Yu, <strong>Yi Ren</strong>, Zhou Zhao,  <span style="color:red">(Oral)</span>, <a href="https://github.com/Rongjiehuang/FastDiff"><img src="https://img.shields.io/github/stars/Rongjiehuang/FastDiff?style=social&amp;label=Code+Stars" alt="" /></a></li>
  <li><code class="language-plaintext highlighter-rouge">NAACL 2022</code> <a href="">A Study of Syntactic Multi-Modality in Non-Autoregressive Machine Translation</a>, Kexun Zhang, Rui Wang, Xu Tan, Junliang Guo, <strong>Yi Ren</strong>, et al.</li>
  <li><code class="language-plaintext highlighter-rouge">ACL 2022</code> <a href="https://arxiv.org/abs/2202.13066">Revisiting Over-Smoothness in Text to Speech</a>, <strong>Yi Ren</strong>, Xu Tan, Tao Qin, et al.</li>
  <li><code class="language-plaintext highlighter-rouge">ACL 2022</code> <a href="https://arxiv.org/abs/2202.13277">Learning the Beauty in Songs: Neural Singing Voice Beautifier</a>, Jinglin Liu, Chengxi Li, <strong>Yi Ren</strong>, et al. | <a href="https://github.com/MoonInTheRiver/NeuralSVB"><img src="https://img.shields.io/github/stars/MoonInTheRiver/NeuralSVB?style=social&amp;label=Code+Stars" alt="" /></a></li>
  <li><code class="language-plaintext highlighter-rouge">ICASSP 2022</code> <a href="https://prosospeech.github.io/">ProsoSpeech: Enhancing Prosody With Quantized Vector Pre-training in Text-to-Speech</a>, <strong>Yi Ren</strong>, et al.</li>
  <li><code class="language-plaintext highlighter-rouge">INTERSPEECH 2021</code> <a href="https://arxiv.org/abs/2106.09317">EMOVIE: A Mandarin Emotion Speech Dataset with a Simple Emotional Text-to-Speech Model</a>, Chenye Cui, <strong>Yi Ren</strong>, et al.</li>
  <li><code class="language-plaintext highlighter-rouge">INTERSPEECH 2021</code> <span style="color:red">(best student paper award candidate)</span> <a href="https://arxiv.org/abs/2106.08507">WSRGlow: A Glow-based Waveform Generative Model for Audio Super-Resolution</a>, Kexun Zhang, <strong>Yi Ren</strong>, Changliang Xu and Zhou Zhao</li>
  <li><code class="language-plaintext highlighter-rouge">ICASSP 2021</code> <a href="https://arxiv.org/abs/2012.09547">Denoising Text to Speech with Frame-Level Noise Modeling</a>, Chen Zhang, <strong>Yi Ren</strong>, Xu Tan, et al. | <a href="https://speechresearch.github.io/denoispeech/"><strong>Project</strong></a></li>
  <li><code class="language-plaintext highlighter-rouge">ACM-MM 2021</code> <a href="https://arxiv.org/pdf/2112.10358">Multi-Singer: Fast Multi-Singer Singing Voice Vocoder With A Large-Scale Corpus</a>, Rongjie Huang, Feiyang Chen, <strong>Yi Ren</strong>, et al. <span style="color:red">(Oral)</span></li>
  <li><code class="language-plaintext highlighter-rouge">IJCAI 2021</code> <a href="https://www.ijcai.org/proceedings/2021/527">FedSpeech: Federated Text-to-Speech with Continual Learning</a>, Ziyue Jiang, <strong>Yi Ren</strong>, et al.</li>
  <li><code class="language-plaintext highlighter-rouge">KDD 2020</code> <a href="https://dl.acm.org/doi/abs/10.1145/3394486.3403249">DeepSinger: Singing Voice Synthesis with Data Mined From the Web</a>, <strong>Yi Ren</strong>, Xu Tan, Tao Qin, et al. | <a href="https://speechresearch.github.io/deepsinger/"><strong>Project</strong></a></li>
  <li><code class="language-plaintext highlighter-rouge">KDD 2020</code> <a href="https://dl.acm.org/doi/abs/10.1145/3394486.3403331">LRSpeech: Extremely Low-Resource Speech Synthesis and Recognition</a>, Jin Xu, Xu Tan, <strong>Yi Ren</strong>, et al. | <a href="https://speechresearch.github.io/lrspeech/"><strong>Project</strong></a></li>
  <li><code class="language-plaintext highlighter-rouge">INTERSPEECH 2020</code> <a href="https://www.isca-speech.org/archive/Interspeech_2020/pdfs/3139.pdf">MultiSpeech: Multi-Speaker Text to Speech with Transformer</a>, Mingjian Chen, Xu Tan, <strong>Yi Ren</strong>, et al. | <a href="https://speechresearch.github.io/multispeech/"><strong>Project</strong></a></li>
  <li><code class="language-plaintext highlighter-rouge">ICML 2019</code> <span style="color:red">(Oral)</span> <a href="https://pdfs.semanticscholar.org/9075/a3e6271e5ef4953491488d1776527e632408.pdf">Almost Unsupervised Text to Speech and Automatic Speech Recognition</a>, <strong>Yi Ren</strong>, Xu Tan, Tao Qin, et al.  | <a href="https://speechresearch.github.io/unsuper/"><strong>Project</strong></a></li>
</ul>



