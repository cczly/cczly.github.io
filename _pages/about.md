---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
--- 

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %} 
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='-about-me'></span> 

I work at Suzhou Institute for advanced research, USTC as an Associate Researcher (master supervisor) now in Suzhou.

My research interest includes Embodied AI and Multimodal Content Generation and Understanding. If you are seeking any form of academic cooperation, please feel free to email me at <strong>cczly@ustc.edu.cn</strong>. There are openings for **PhD/master's students and interns**.

I have published in several journals, such as IEEE Transactions on Image Processing, IEEE Transactions on Multimedia, IEEE Transactions on Circuits and Systems for Video Technology, Pattern Recognition, and IEEE Transactions on Instrumentation and Measurement, and top conferences, such as ICML, CVPR, ICCV, AAAI, and ACM MM. I am nominated for the Shanghai Computer Society's Outstanding Doctoral Dissertation Award, received the honor of Outstanding Graduate Student of Shanghai, and was titled as an Outstanding Postdoctoral Fellow in Jiangsu Province, China.

<span class='anchor' id='-news'></span>

# ğŸ”¥ News 
- *2025.11*: &nbsp;ğŸ‰ A conference paper is accepted by AAAI2026.
- *2025.05*: &nbsp;ğŸ‰ Oure paper is selected as Oral by ICME2025.
- *2025.05*: &nbsp;ğŸ‰ Awarded the Huawei Spark Award for technical contributions to challenging problems in artificial intelligence.
- *2025.05*: &nbsp;ğŸ‰ A conference paper is accepted by ICML2025.
- *2025.04*: &nbsp;ğŸ‰ A conference paper completed by undergraduate interns is accepted by IEEE IJCNN2025.
- *2025.03*: &nbsp;ğŸ‰ A conference paper is accepted by IEEE ICME2025.
- *2025.01*: &nbsp;ğŸ‰ Promoted to Associate Researcher (master supervisor).

# ğŸ– Honors and Awards
- **2025**:&nbsp;ğŸ‰ Huawei Spark Prizeï¼ˆåä¸ºç«èŠ±å¥–ï¼‰.
- **2023**:&nbsp;ğŸ‰ Outstanding Postdoctoral Fellow in Jiangsu Province.
- **2022**:&nbsp;ğŸ‰ Outstanding Graduate Student of Shanghai. 
- **2022**:&nbsp;ğŸ‰ Shanghai Computer Society's Outstanding Doctoral Dissertation Award (Nominated).
- **2021**:&nbsp;ğŸ‰ Doctoral National Scholarship.
- **2020**:&nbsp;ğŸ‰ Doctoral National Scholarship.

<span class='anchor' id='-research-highlights'></span>

# ğŸ”Research Highlights
- **Embodied AI**: &nbsp; Including visual-language-action model, scence geneation, dynamic rendering. 
- **Physics-informed Visual Computing**: &nbsp; Including novel view synthesis, physics-informed dynamic and data-driven inverse problem. 
- **Multimodal Content Generation and Understanding**: &nbsp; Including humanoid motion generation, video generation, low-rank learning of large models and mixture of expert systems.


<span class='anchor' id='-publications'></span>

# ğŸ“ Publications 
(*, Corresponding Author)

  
<ul>

  <li><code class="language-plaintext highlighter-rouge" 
      style="background-color: #001f3f; 
             color: #ffffff; 
             padding: 2px 4px; 
             border-radius: 4px;">
  AAAI2026
</code><code class="language-plaintext highlighter-rouge" 
      style="background-color:rgb(236, 236, 233); 
             color:rgb(76, 59, 226); 
             padding: 2px 4px; 
             border-radius: 4px;">
 <a href="https://github.com/SCAILab-USTC/Physics-Informed-Deformable-Gaussian-Splatting">Code</a> 
</code> 
 <a href="https://arxiv.org/abs/2511.06299">Physics-Informed Deformable Gaussian Splatting: Towards Unified Constitutive Laws for Time-Evolving Material Field</a>. <strong>Haoqin Hong</strong>, <strong>Ding Fan</strong>, Fubin Dou, Zhi-Li Zhou, Haoran Sun, <strong>Congcong Zhu*</strong>, Jingrun Chen*
 </li>

  <li><code class="language-plaintext highlighter-rouge" 
      style="background-color: #001f3f; 
             color: #ffffff; 
             padding: 2px 4px; 
             border-radius: 4px;">
  ICML2025
</code><code class="language-plaintext highlighter-rouge" 
      style="background-color:rgb(236, 236, 233); 
             color:rgb(76, 59, 226); 
             padding: 2px 4px; 
             border-radius: 4px;">
 <a href="https://github.com/SCAILab-USTC/PITA">Code</a> 
</code> 
 <a href="https://arxiv.org/pdf/2505.10930">Physics-informed Temporal Alignment for Auto-regressive PDE Foundation Models</a>. <strong>Congcong Zhu</strong>, <strong>Xiaoyan Xu</strong>, Jiayue Han, Jingrun Chen*
</li>


  <li><code class="language-plaintext highlighter-rouge" 
      style="background-color: #001f3f; 
             color: #ffffff; 
             padding: 2px 4px; 
             border-radius: 4px;"> 
  IJCNN2025
</code> <a href="https://vimeo.com/1095993656">Toward Invisible Region Restoration for Single-View 3D Face Reconstruction</a>. Zhijing Cheng, YuQing Wen, <strong>Congcong Zhu*</strong>, Rui Du* </li>

  <li><code class="language-plaintext highlighter-rouge" 
      style="background-color: #001f3f; 
             color: #ffffff; 
             padding: 2px 4px; 
             border-radius: 4px;">
  ICME2025 (Oral)
</code> <code class="language-plaintext highlighter-rouge" 
      style="background-color:rgb(236, 236, 233); 
             color:rgb(76, 59, 226); 
             padding: 2px 4px; 
             border-radius: 4px;">
 <a href="https://github.com/SCAILab-USTC/STSA">Code</a> 
</code> <a href="https://arxiv.org/abs/2503.23039">STSA: Spatial-Temporal Semantic Alignment for Visual Dubbing</a>. Zijun Ding
, Mingdie Xiong
, <strong>Congcong Zhu*</strong>
, Jingrun Chen </li>

  <li><code class="language-plaintext highlighter-rouge" 
      style="background-color: #001f3f; 
             color: #ffffff; 
             padding: 2px 4px; 
             border-radius: 4px;">
  IEEE TCSVT
</code>
 <a href="https://ieeexplore.ieee.org/abstract/document/10583942">Toward Quantifiable Face Age Transformation under Attribute Unbias</a>. Ling Lin, Tao Wang, Hao Liu, <strong>Congcong Zhu*</strong>, Jingrun Chen</li>

  <li><code class="language-plaintext highlighter-rouge" 
      style="background-color: #001f3f; 
             color: #ffffff; 
             padding: 2px 4px; 
             border-radius: 4px;">
  ICASSP2024
</code> <a href="https://ieeexplore.ieee.org/abstract/document/10448304">Toward Quantifiable Face age Transformation</a>. Ling Lin, <strong>Congcong Zhu*</strong>, Lin Zhou, Jingrun Chen</li>

  <li><code class="language-plaintext highlighter-rouge" 
      style="background-color: #001f3f; 
             color: #ffffff; 
             padding: 2px 4px; 
             border-radius: 4px;">
  IEEE TIP
</code> <a href="https://ieeexplore.ieee.org/abstract/document/10462910">HeadDiff: Exploring Rotation Uncertainty With Diffusion Models for Head Pose Estimation</a>. Yaoxing Wang, Hao Liu, Yaowei Feng, Zhendong Li, Xiangjuan Wu, <strong>Congcong Zhu</strong></li>
  <li><code class="language-plaintext highlighter-rouge" 
      style="background-color: #001f3f; 
             color: #ffffff; 
             padding: 2px 4px; 
             border-radius: 4px;">
  IEEE TIM
</code> <a href="https://ieeexplore.ieee.org/abstract/document/10049177">A multi-scale feature fusion network with cascaded supervision for cross-scene crowd counting</a>. Xinfeng Zhang, Lina Han, Wencong Shan, Xiaohu Wang, Shuhan Chen, <strong>Congcong Zhu</strong>, Bin Li</li>
  <li><code class="language-plaintext highlighter-rouge" 
      style="background-color: #001f3f; 
             color: #ffffff; 
             padding: 2px 4px; 
             border-radius: 4px;">
  IEEE TMM
</code> <a href="https://ieeexplore.ieee.org/abstract/document/9911664">Multi-Sourced Knowledge Integration for Robust Self-Supervised Facial Landmark Tracking</a>. <strong>Congcong Zhu</strong>, Xiaoqiang Li, Jide Li, Songmin Dai, Weiqin Tong </li>
  <li><code class="language-plaintext highlighter-rouge" 
      style="background-color: #001f3f; 
             color: #ffffff; 
             padding: 2px 4px; 
             border-radius: 4px;">
  PR
</code> <a href="https://arxiv.org/pdf/2112.10087">Reasoning structural relation for occlusion-robust facial landmark localization</a>. <strong>Congcong Zhu</strong>, Xiaoqiang Li, Jide Li, Songmin Dai, Weiqin Tong </li>
  <li><code class="language-plaintext highlighter-rouge" 
      style="background-color: #001f3f; 
             color: #ffffff; 
             padding: 2px 4px; 
             border-radius: 4px;">
  CVPR2022 (Oral)
</code> <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhu_Occlusion-Robust_Face_Alignment_Using_a_Viewpoint-Invariant_Hierarchical_Network_Architecture_CVPR_2022_paper.pdf">Occlusion-robust face alignment using a viewpoint-invariant hierarchical network architecture</a>. <strong>Congcong Zhu</strong>, Xintong Wan, Shaorong Xie, Xiaoqiang Li, Yinzheng Gu </li>
  <li><code class="language-plaintext highlighter-rouge" 
      style="background-color: #001f3f; 
             color: #ffffff; 
             padding: 2px 4px; 
             border-radius: 4px;">
  ICCV2021
</code> <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Zhu_Improving_Robustness_of_Facial_Landmark_Detection_by_Defending_Against_Adversarial_ICCV_2021_paper.pdf">Improving robustness of facial landmark detection by defending against adversarial attacks</a>. <strong>Congcong Zhu</strong>, Xiaoqiang Li, Jide Li, Songmin Dai </li>
  <li><code class="language-plaintext highlighter-rouge" 
      style="background-color: #001f3f; 
             color: #ffffff; 
             padding: 2px 4px; 
             border-radius: 4px;">
  PR
</code> <a href="https://www.sciencedirect.com/science/article/pii/S0031320320301576">Learning spatial-temporal deformable networks for unconstrained face alignment and tracking in videos</a>. Hongyu Zhu, Hao Liu, <strong>Congcong Zhu</strong>, Zongyong Deng, Xuehong Sun. </li>
  <li><code class="language-plaintext highlighter-rouge" 
      style="background-color: #001f3f; 
             color: #ffffff; 
             padding: 2px 4px; 
             border-radius: 4px;">
  ACM MM2020
</code> <a href="https://dl.acm.org/doi/abs/10.1145/3394171.3413993">Spatial-temporal knowledge integration: Robust self-supervised facial landmark tracking</a>. <strong>Congcong Zhu</strong>, Xiaoqiang Li, Jide Li, Guangtai Ding, Weiqin Tong. </li>
  <li><code class="language-plaintext highlighter-rouge" 
      style="background-color: #001f3f; 
             color: #ffffff; 
             padding: 2px 4px; 
             border-radius: 4px;">
  AAAI2020 (Spotlight)
</code> <a href="https://ojs.aaai.org/index.php/AAAI/article/view/7011">Towards Omni-Supervised Face Alignment for Large Scale Unlabeled Videos</a>. <strong>Congcong Zhu</strong>, Hao Liu, Zhenhua Yu, Xuehong Sun. </li>
</ul>

# Acknowledgement
Thanks for this convenient [template](https://github.com/RayeRen/acad-homepage.github.io).
